[student@workstation ~]$ lab start  deploy-expand

Starting lab.

 · Checking lab systems ................................................................................................................................................................................. SUCCESS
 · Create example folder ................................................................................................................................................................................ SUCCESS
 · Create service file .................................................................................................................................................................................. SUCCESS
 · Create example file .................................................................................................................................................................................. SUCCESS
 · Copy cluster public key to clienta ................................................................................................................................................................... SUCCESS
 · Update service specification ......................................................................................................................................................................... SUCCESS
 · Remove OSDs .......................................................................................................................................................................................... SUCCESS
 · Remove the unmanaged daemons ......................................................................................................................................................................... SUCCESS
 · Zap devices on serverc ............................................................................................................................................................................... SUCCESS
 · Zap devices on serverd ............................................................................................................................................................................... SUCCESS
 · Zap devices on servere ............................................................................................................................................................................... SUCCESS

[student@workstation ~]$ ssh admin@clienta
Activate the web console with: systemctl enable --now cockpit.socket

This system is not registered to Red Hat Insights. See https://cloud.redhat.com/
To register this system, run: insights-client --register

Last login: Fri Mar 18 10:45:42 2022 from 172.25.250.9
[admin@clienta ~]$ sudo cephadm shell
Inferring fsid 1bcaf430-a6c9-11ec-918b-52540000fa0c
Inferring config /var/lib/ceph/1bcaf430-a6c9-11ec-918b-52540000fa0c/mon.clienta/config
Using recent ceph image registry.redhat.io/rhceph/rhceph-5-rhel8@sha256:6306de945a6c940439ab584aba9b622f2aa6222947d3d4cde75a4b82649a47ff
[ceph: root@clienta /]# 
[ceph: root@clienta /]# 
[ceph: root@clienta /]# ceph orch device ls 
Hostname                 Path      Type  Serial                Size   Health   Ident  Fault  Available  
clienta.lab.example.com  /dev/vdb  hdd   ddad3880-4620-491b-8  10.7G  Unknown  N/A    N/A    Yes        
clienta.lab.example.com  /dev/vdc  hdd   4102f95f-3b33-433a-8  10.7G  Unknown  N/A    N/A    Yes        
clienta.lab.example.com  /dev/vdd  hdd   c12b6ce3-61b7-437b-9  10.7G  Unknown  N/A    N/A    Yes        
clienta.lab.example.com  /dev/vde  hdd   456408d8-7a36-4621-a  10.7G  Unknown  N/A    N/A    Yes        
clienta.lab.example.com  /dev/vdf  hdd   b81e4d4c-e3e2-443a-b  10.7G  Unknown  N/A    N/A    Yes        
serverc.lab.example.com  /dev/vdc  hdd   94e97201-ca2d-42fb-9  10.7G  Unknown  N/A    N/A    Yes        
serverc.lab.example.com  /dev/vdd  hdd   94c84103-942f-415d-8  10.7G  Unknown  N/A    N/A    Yes        
serverc.lab.example.com  /dev/vde  hdd   3865d719-d8ab-4aa6-9  10.7G  Unknown  N/A    N/A    Yes        
serverc.lab.example.com  /dev/vdf  hdd   dc689236-5566-4dcb-b  10.7G  Unknown  N/A    N/A    Yes        
serverc.lab.example.com  /dev/vdb  hdd   8155e00d-99b3-47f9-8  10.7G  Unknown  N/A    N/A    No         
serverd.lab.example.com  /dev/vdc  hdd   e00ad7fe-c0e5-4681-b  10.7G  Unknown  N/A    N/A    Yes        
serverd.lab.example.com  /dev/vdd  hdd   7fce5969-9e57-4c62-8  10.7G  Unknown  N/A    N/A    Yes        
serverd.lab.example.com  /dev/vde  hdd   0f70d52f-96fc-460d-b  10.7G  Unknown  N/A    N/A    Yes        
serverd.lab.example.com  /dev/vdf  hdd   7f5ad5ca-78a3-4bff-b  10.7G  Unknown  N/A    N/A    Yes        
serverd.lab.example.com  /dev/vdb  hdd   7e51ff2e-3459-4087-9  10.7G  Unknown  N/A    N/A    No         
servere.lab.example.com  /dev/vdc  hdd   311ae7a3-f0a0-450d-a  10.7G  Unknown  N/A    N/A    Yes        
servere.lab.example.com  /dev/vdd  hdd   eb221274-3826-4a84-b  10.7G  Unknown  N/A    N/A    Yes        
servere.lab.example.com  /dev/vde  hdd   5e38820a-4f58-4572-a  10.7G  Unknown  N/A    N/A    Yes        
servere.lab.example.com  /dev/vdf  hdd   50e70716-d4e4-4ac3-8  10.7G  Unknown  N/A    N/A    Yes        
servere.lab.example.com  /dev/vdb  hdd   2fb0ef3f-4aae-49d7-9  10.7G  Unknown  N/A    N/A    No         
[ceph: root@clienta /]# exit
exit
[admin@clienta ~]$ sudo ls /root/
-  expand-osd
[admin@clienta ~]$ sudo ls /root/expand-osd
osd_service.yml  osd_spec.yml
[admin@clienta ~]$ 
[admin@clienta ~]$ 
[admin@clienta ~]$ sudo cephadm shell --mount /root/expand-osd/osd_spec.yml 
Inferring fsid 1bcaf430-a6c9-11ec-918b-52540000fa0c
Inferring config /var/lib/ceph/1bcaf430-a6c9-11ec-918b-52540000fa0c/mon.clienta/config
Using recent ceph image registry.redhat.io/rhceph/rhceph-5-rhel8@sha256:6306de945a6c940439ab584aba9b622f2aa6222947d3d4cde75a4b82649a47ff
[ceph: root@clienta /]# 
[ceph: root@clienta /]# 
[ceph: root@clienta /]# cd /mnt/
[ceph: root@clienta mnt]# ls
osd_spec.yml
[ceph: root@clienta mnt]# cp osd_spec.yml /var/lib/ceph/osd/
[ceph: root@clienta mnt]# cat /var/lib/ceph/osd/osd_spec.yml 
service_type: osd
service_id: default_drive_group
placement:
  hosts:
    - serverc.lab.example.com
    - serverd.lab.example.com
    - servere.lab.example.com
data_devices:
  paths:
    - /dev/vdb
    - /dev/vdc
    - /dev/vdd
[ceph: root@clienta mnt]# 
[ceph: root@clienta mnt]# 
[ceph: root@clienta mnt]# 
[ceph: root@clienta mnt]# ceph orch apply -i /var/lib/ceph/osd/osd_spec.yml
Scheduled osd.default_drive_group update...
[ceph: root@clienta mnt]#    
[ceph: root@clienta mnt]# 
[ceph: root@clienta mnt]# ceph orch device ls --hostname=servere.lab.example.com 
Hostname                 Path      Type  Serial                Size   Health   Ident  Fault  Available  
servere.lab.example.com  /dev/vdc  hdd   311ae7a3-f0a0-450d-a  10.7G  Unknown  N/A    N/A    Yes        
servere.lab.example.com  /dev/vdd  hdd   eb221274-3826-4a84-b  10.7G  Unknown  N/A    N/A    Yes        
servere.lab.example.com  /dev/vde  hdd   5e38820a-4f58-4572-a  10.7G  Unknown  N/A    N/A    Yes        
servere.lab.example.com  /dev/vdf  hdd   50e70716-d4e4-4ac3-8  10.7G  Unknown  N/A    N/A    Yes        
servere.lab.example.com  /dev/vdb  hdd   2fb0ef3f-4aae-49d7-9  10.7G  Unknown  N/A    N/A    No         
[ceph: root@clienta mnt]# 
[ceph: root@clienta mnt]# 
[ceph: root@clienta mnt]# ceph orch daemon add osd servere.lab.example.com:/dev/vde
Created osd(s) 9 on host 'servere.lab.example.com'
[ceph: root@clienta mnt]# 
[ceph: root@clienta mnt]# 
[ceph: root@clienta mnt]# ceph orch daemon add osd servere.lab.example.com:/dev/vdf
Created osd(s) 10 on host 'servere.lab.example.com'
[ceph: root@clienta mnt]# 
[ceph: root@clienta mnt]# 
[ceph: root@clienta mnt]# ceph orch device ls --hostname=servere.lab.example.com 
Hostname                 Path      Type  Serial                Size   Health   Ident  Fault  Available  
servere.lab.example.com  /dev/vdb  hdd   2fb0ef3f-4aae-49d7-9  10.7G  Unknown  N/A    N/A    No         
servere.lab.example.com  /dev/vdc  hdd   311ae7a3-f0a0-450d-a  10.7G  Unknown  N/A    N/A    No         
servere.lab.example.com  /dev/vdd  hdd   eb221274-3826-4a84-b  10.7G  Unknown  N/A    N/A    No         
servere.lab.example.com  /dev/vde  hdd   5e38820a-4f58-4572-a  10.7G  Unknown  N/A    N/A    No         
servere.lab.example.com  /dev/vdf  hdd   50e70716-d4e4-4ac3-8  10.7G  Unknown  N/A    N/A    No         
[ceph: root@clienta mnt]# 
[ceph: root@clienta mnt]# 
[ceph: root@clienta mnt]# ceph status
  cluster:
    id:     1bcaf430-a6c9-11ec-918b-52540000fa0c
    health: HEALTH_WARN
            2 slow ops, oldest one blocked for 108 sec, mon.servere has slow ops
 
  services:
    mon: 4 daemons, quorum serverc.lab.example.com,clienta,serverd,servere (age 18m)
    mgr: serverc.lab.example.com.vatlmp(active, since 20m), standbys: serverd.jjhxyg, clienta.dulidq, servere.zrflgk
    osd: 11 osds: 11 up (since 21s), 11 in (since 38s)
    rgw: 2 daemons active (2 hosts, 1 zones)
 
  data:
    pools:   5 pools, 105 pgs
    objects: 189 objects, 4.9 KiB
    usage:   148 MiB used, 110 GiB / 110 GiB avail
    pgs:     105 active+clean
 
  io:
    recovery: 0 B/s, 0 objects/s
 
[ceph: root@clienta mnt]# ceph osd tree
ID  CLASS  WEIGHT   TYPE NAME         STATUS  REWEIGHT  PRI-AFF
-1         0.10776  root default                               
-5         0.02939      host serverc                           
 2    hdd  0.00980          osd.2         up   1.00000  1.00000
 5    hdd  0.00980          osd.5         up   1.00000  1.00000
 8    hdd  0.00980          osd.8         up   1.00000  1.00000
-7         0.02939      host serverd                           
 1    hdd  0.00980          osd.1         up   1.00000  1.00000
 4    hdd  0.00980          osd.4         up   1.00000  1.00000
 7    hdd  0.00980          osd.7         up   1.00000  1.00000
-3         0.04898      host servere                           
 0    hdd  0.00980          osd.0         up   1.00000  1.00000
 3    hdd  0.00980          osd.3         up   1.00000  1.00000
 6    hdd  0.00980          osd.6         up   1.00000  1.00000
 9    hdd  0.00980          osd.9         up   1.00000  1.00000
10    hdd  0.00980          osd.10        up   1.00000  1.00000
[ceph: root@clienta mnt]# ceph osd df 
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS
 2    hdd  0.00980   1.00000   10 GiB   20 MiB  2.7 MiB   0 B   18 MiB   10 GiB  0.20  1.51   33      up
 5    hdd  0.00980   1.00000   10 GiB   11 MiB  2.7 MiB   0 B  8.6 MiB   10 GiB  0.11  0.84   36      up
 8    hdd  0.00980   1.00000   10 GiB   11 MiB  2.7 MiB   0 B  8.2 MiB   10 GiB  0.11  0.81   36      up
 1    hdd  0.00980   1.00000   10 GiB   20 MiB  2.7 MiB   0 B   18 MiB   10 GiB  0.20  1.51   35      up
 4    hdd  0.00980   1.00000   10 GiB   11 MiB  2.7 MiB   0 B  8.4 MiB   10 GiB  0.11  0.83   31      up
 7    hdd  0.00980   1.00000   10 GiB   11 MiB  2.7 MiB   0 B  8.2 MiB   10 GiB  0.11  0.81   39      up
 0    hdd  0.00980   1.00000   10 GiB   20 MiB  2.7 MiB   0 B   17 MiB   10 GiB  0.20  1.49   21      up
 3    hdd  0.00980   1.00000   10 GiB   11 MiB  2.7 MiB   0 B  8.8 MiB   10 GiB  0.11  0.85   23      up
 6    hdd  0.00980   1.00000   10 GiB   11 MiB  2.7 MiB   0 B  8.1 MiB   10 GiB  0.11  0.80   19      up
 9    hdd  0.00980   1.00000   10 GiB   10 MiB  2.7 MiB   0 B  7.8 MiB   10 GiB  0.10  0.77   20      up
10    hdd  0.00980   1.00000   10 GiB   11 MiB  2.7 MiB   0 B  7.9 MiB   10 GiB  0.10  0.79   22      up
                       TOTAL  110 GiB  149 MiB   30 MiB   0 B  119 MiB  110 GiB  0.13                   
MIN/MAX VAR: 0.77/1.51  STDDEV: 0.04
[ceph: root@clienta mnt]# 
[ceph: root@clienta mnt]# 
[ceph: root@clienta mnt]# exit
exit
[admin@clienta ~]$ exit
logout
Connection to clienta closed.
[student@workstation ~]$ lab finish deploy-expand

Finishing lab.

 · Checking lab systems ................................................................................................................................................................................. SUCCESS
 · Delete temporary exercise directory .................................................................................................................................................................. SUCCESS
 · Stop the OSD daemon for servere /dev/vde ............................................................................................................................................................. SUCCESS
 · Stop the OSD daemon for servere /dev/vdf ............................................................................................................................................................. SUCCESS
 · Cleanup OSD IDs for devices .......................................................................................................................................................................... SUCCESS
 · Zap devices on servere ............................................................................................................................................................................... SUCCESS
 · Clean the OSDs from CRUSH map ........................................................................................................................................................................ SUCCESS
 · Remove the unmanaged daemons ......................................................................................................................................................................... SUCCESS

[student@workstation ~]$ 

